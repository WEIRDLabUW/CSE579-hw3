{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjzQlyjt8SIh"
   },
   "source": [
    "# Setup environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Rls1RyNOpiO"
   },
   "outputs": [],
   "source": [
    "# Perform git clone as follows\n",
    "\n",
    "!git clone https://github.com/WEIRDLabUW/CSE579-hw3.git\n",
    "!cp -r cse542_sp24_hw2/* .\n",
    "\n",
    "# !NOTE!: Once you are done, copy your implementation of policy gradient, actor critic and\n",
    "# in the notebook here back to the python script\n",
    "# when submiting your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 42588,
     "status": "ok",
     "timestamp": 1731457358966,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "GQcORlUOOpiQ",
    "outputId": "13c4e6ba-4fb6-46d5-af55-b553edf4b711"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf\n",
    "!pip install setuptools wheel\n",
    "!pip install gym==0.26.2\n",
    "!pip install gymnasium-robotics[mujoco-py]\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install gym-notices==0.0.8\n",
    "!pip install matplotlib\n",
    "!pip install mujoco\n",
    "!pip install free-mujoco-py\n",
    "!pip install pybullet\n",
    "! pip install tqdm diffusers\n",
    "import os\n",
    "os.environ['LD_PRELOAD']=':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 69809,
     "status": "ok",
     "timestamp": 1731457434929,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "k5XVqztZVDZv",
    "outputId": "4a7fa4da-c045-4cf5-c062-f5c0f669f4a9"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Reacher-v2\")\n",
    "env.get_body_com(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1731457468638,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "y-YiBU9aOpiQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_PRELOAD']=':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from utils import DeterministicDynamicsModel, set_random_seed, reward_fn_reacher, ReplayBuffer\n",
    "from rollouts import evaluate\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JX6q5XP8Xb5"
   },
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1731458622919,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "Huv5J43cOpiQ"
   },
   "outputs": [],
   "source": [
    "def rollout_model(\n",
    "        model,\n",
    "        initial_states,\n",
    "        actions,\n",
    "        horizon,\n",
    "        reward_fn):\n",
    "    # Collect the following data\n",
    "    all_states = []\n",
    "    all_rewards = []\n",
    "    curr_state = initial_states # Starting from the initial state\n",
    "    #========== TODO: start ==========\n",
    "    # Hint1: concatenate current state and action pairs as the input for the model and predict the next observation\n",
    "    # Hint2: get the predicted reward using reward_fn()\n",
    "\n",
    "\n",
    "\n",
    "    #========== TODO: end ==========\n",
    "    all_states_full = torch.cat([state[:, None, :] for state in all_states], dim=1).cpu().detach().numpy()\n",
    "    all_rewards_full = torch.cat(all_rewards, dim=-1).cpu().detach().numpy()    \n",
    "    return all_states_full, all_rewards_full\n",
    "\n",
    "\n",
    "def get_ensemble_rewards(model, state_repeats, random_actions, horizon, reward_fn):\n",
    "    \"\"\"This method will generate the average reward rolled out over the ensemble of models\n",
    "\n",
    "    Args:\n",
    "        model List: The list of models\n",
    "        state_repeats: The initial state repeated over the num random action sequence dimension\n",
    "        random_actions: The random actions to be taken\n",
    "        horizon: How long to roll out the model\n",
    "        reward_fn: Used to get the reward\n",
    "    \"\"\"\n",
    "    #========== TODO: start ==========\n",
    "    # For each model in the list of models, rollout the model and get the rewards using the rollout_model\n",
    "    # function. Take the mean of the rewards over each time step to get the average reward to return for\n",
    "    # the passed action sequence.\n",
    "\n",
    "\n",
    "\n",
    "    #========== TODO: end ==========\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "def plan_model_random_shooting(env, state, ac_size, horizon, model, reward_fn, n_samples_mpc=100):\n",
    "    #========== TODO: start ==========\n",
    "    # Hint1: randomly sample actions in the action space\n",
    "    # Hint2: rollout model based on current state and random action, select the best action that maximize the sum of the reward\n",
    "    # Rolling forward random actions through the model\n",
    "\n",
    "    \n",
    "    \n",
    "    #========== TODO: end ==========\n",
    "    return best_ac, random_actions[best_ac_idx]\n",
    "\n",
    "\n",
    "def plan_model_mppi(env, state, ac_size, horizon, model, reward_fn, n_samples_mpc=100, n_iter_mppi=10, gaussian_noise_scales=[1.0, 1.0, 0.5, 0.5, 0.2, 0.2, 0.1, 0.1, 0.01, 0.01]):\n",
    "    assert len(gaussian_noise_scales) == n_iter_mppi\n",
    "    # Rolling forward random actions through the model\n",
    "    state_repeats = torch.from_numpy(np.repeat(state[None], n_samples_mpc, axis=0)).cuda()\n",
    "    # Sampling random actions in the range of the action space\n",
    "    random_actions = torch.FloatTensor(n_samples_mpc, horizon, ac_size).uniform_(env.action_space.low[0], env.action_space.high[0]).cuda().float()\n",
    "    # Rolling forward through the mdoel for horizon steps\n",
    "    if not isinstance(model, list):\n",
    "        all_states, all_rewards = rollout_model(model, state_repeats, random_actions, horizon, reward_fn)\n",
    "    else:\n",
    "        # NOTE: Implement this branch in part 4 of the writeup, not in part 3.\n",
    "        pass\n",
    "\n",
    "\n",
    "    all_returns = all_rewards.sum(axis=-1)\n",
    "    # Take first action from best trajectory\n",
    "    best_ac_idx = np.argmax(all_rewards.sum(axis=-1))\n",
    "    best_ac = random_actions[best_ac_idx, 0] # Take the first action from the best trajectory\n",
    "\n",
    "    # Run through a few iterations of MPPI\n",
    "\n",
    "    #========== TODO: start ==========\n",
    "    # Hint1: Compute weights based on exponential of returns\n",
    "    # Hint2: sample actions based on the weight, and compute average return over models\n",
    "    # Hint3: if model type is a list, then implement ensemble mppi\n",
    "\n",
    "    for iter in range(n_iter_mppi):\n",
    "        # Weight trajectories by exponential of returns\n",
    "\n",
    "\n",
    "        # Rolling forward through the model for horizon steps (or ensemble) to update the rewards\n",
    "        if not isinstance(model, list):\n",
    "            # Rolling forward through the mdoel for horizon steps\n",
    "            pass # Fill this in\n",
    "        else:\n",
    "            # NOTE: Implement this branch in part 4 of the writeup, not in part 3.\n",
    "            pass\n",
    "    #========== TODO: end ==========\n",
    "\n",
    "    # Finally take first action from best trajectory\n",
    "    best_ac_idx = np.argmax(all_rewards.sum(axis=-1))\n",
    "    best_ac = random_actions[best_ac_idx, 0] # Take the first action from the best trajectory\n",
    "    return best_ac, random_actions[best_ac_idx]\n",
    "\n",
    "\n",
    "\n",
    "def train_single(num_epochs, num_batches,batch_size, model, optimizer, replay_buffer):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            t1_observations, t1_actions, _, t1_next_observations, _ = replay_buffer.sample(batch_size)\n",
    "            oa_in = torch.cat([t1_observations, t1_actions], dim=-1)\n",
    "\n",
    "            next_o_pred = model(oa_in)\n",
    "            loss = loss_fn(next_o_pred, t1_next_observations)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def train_model(model, replay_buffer, optimizer, num_epochs=500, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train a single model with supervised learning\n",
    "    \"\"\"\n",
    "    idxs = np.array(range(len(replay_buffer)))\n",
    "    num_batches = len(idxs) // batch_size\n",
    "    if not isinstance(model, list):\n",
    "        train_single(num_epochs, num_batches, batch_size, model, optimizer, replay_buffer)\n",
    "    else:\n",
    "        #========== TODO: start ==========\n",
    "        # Write code to train the ensemble of models.\n",
    "        # Hint1: Each model should have a different batch size for each model\n",
    "        # Hint2: check out how we define optimizer and model for ensemble models.\n",
    "        # During training, each model should have their individual optimizer to increase diversity.\n",
    "        # Hint3: You can use the train_single function to train each model.\n",
    "        pass\n",
    "\n",
    "        #========== TODO: end ==========\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLQKYfHD8D4W"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1731458623073,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "5YDnB7JLOpiR"
   },
   "outputs": [],
   "source": [
    "def planning_agent(env, o_for_agent, model, reward_fn, plan_mode, mpc_horizon=None, n_samples_mpc=None):\n",
    "    if plan_mode == 'random':\n",
    "        # Taking random actions\n",
    "        action = torch.Tensor(env.action_space.sample()[None]).cuda()\n",
    "    elif plan_mode == 'random_mpc':\n",
    "        # Taking actions via random shooting + MPC\n",
    "        action, _ = plan_model_random_shooting(env, o_for_agent, env.action_space.shape[0], mpc_horizon, model,\n",
    "                                               reward_fn, n_samples_mpc=n_samples_mpc)\n",
    "    elif plan_mode == 'mppi':\n",
    "        action, _ = plan_model_mppi(env, o_for_agent, env.action_space.shape[0], mpc_horizon, model, reward_fn,\n",
    "                                    n_samples_mpc=n_samples_mpc)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Other planning methods not implemented\")\n",
    "    return action\n",
    "\n",
    "def collect_traj_MBRL(\n",
    "        env,\n",
    "        model,\n",
    "        plan_mode,\n",
    "        replay_buffer=None,\n",
    "        device='cuda:0',\n",
    "        episode_length=math.inf,\n",
    "        reward_fn=None, #Reward function to evaluate\n",
    "        render=False,\n",
    "        mpc_horizon=None,\n",
    "        n_samples_mpc=None\n",
    "):\n",
    "    # Collect the following data\n",
    "    raw_obs = []\n",
    "    raw_next_obs = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    images = []\n",
    "\n",
    "    path_length = 0\n",
    "    o, _ = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    while path_length < episode_length:\n",
    "        o_for_agent = o\n",
    "\n",
    "        # Using the planning agent to take actions\n",
    "        action = planning_agent(env, o_for_agent, model, reward_fn, plan_mode, mpc_horizon=mpc_horizon, n_samples_mpc=n_samples_mpc)\n",
    "        if len(action.shape) == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "\n",
    "        # Step the simulation forward\n",
    "        next_o, r, done, trunc,  env_info = env.step(copy.deepcopy(action))\n",
    "        done = done or trunc\n",
    "        if replay_buffer is not None:\n",
    "            replay_buffer.add(o,\n",
    "                            action,\n",
    "                            r,\n",
    "                            next_o,\n",
    "                            done)\n",
    "        # Render the environment\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        raw_obs.append(o)\n",
    "        raw_next_obs.append(next_o)\n",
    "        actions.append(action)\n",
    "        rewards.append(r)\n",
    "        dones.append(done)\n",
    "        path_length += 1\n",
    "        if done:\n",
    "            break\n",
    "        o = next_o\n",
    "\n",
    "    # Prepare the items to be returned\n",
    "    observations = np.array(raw_obs)\n",
    "    next_observations = np.array(raw_next_obs)\n",
    "    actions = np.array(actions)\n",
    "    if len(actions.shape) == 1:\n",
    "        actions = np.expand_dims(actions, 1)\n",
    "    rewards = np.array(rewards)\n",
    "    if len(rewards.shape) == 1:\n",
    "        rewards = rewards.reshape(-1, 1)\n",
    "    dones = np.array(dones).reshape(-1, 1)\n",
    "\n",
    "    # Return in the following format\n",
    "    return dict(\n",
    "        observations=observations,\n",
    "        next_observations=next_observations,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        dones=np.array(dones).reshape(-1, 1),\n",
    "        images=np.array(images)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1731458623283,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "oRhQEafkOpiR"
   },
   "outputs": [],
   "source": [
    "# Training loop for policy gradient\n",
    "def simulate_mbrl(env, model, plan_mode, num_epochs=200, max_path_length=200, mpc_horizon=10, n_samples_mpc=200,\n",
    "                  batch_size=100, num_agent_train_epochs_per_iter=1000, capacity=100000, num_traj_per_iter=100, gamma=0.99, print_freq=10, device = \"cuda\", reward_fn=None):\n",
    "\n",
    "    # Set up optimizer and replay buffer\n",
    "    if not isinstance(model, list):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    else:\n",
    "        print('Initialize separate optimizers for ensemble mbrl')\n",
    "\n",
    "        optimizer = []\n",
    "        for model_id, curr_model in enumerate(model):\n",
    "            optimizer.append(optim.Adam(curr_model.parameters(), lr=(model_id+1)*1e-4)) # use separate optimizer and apply different learning rate to each model\n",
    "\n",
    "    replay_buffer = ReplayBuffer(obs_size = env.observation_space.shape[0],\n",
    "                                 action_size = env.action_space.shape[0],\n",
    "                                 capacity=capacity,\n",
    "                                 device=device)\n",
    "\n",
    "    # Iterate through data collection and planning loop\n",
    "    for iter_num in range(num_epochs):\n",
    "        # Sampling trajectories\n",
    "        sample_trajs = []\n",
    "        if iter_num == 0:\n",
    "            # Seed with some initial data, collecting with mode random\n",
    "            for it in range(num_traj_per_iter):\n",
    "                sample_traj = collect_traj_MBRL(env=env,\n",
    "                                                model=model,\n",
    "                                                plan_mode='random',\n",
    "                                                replay_buffer=replay_buffer,\n",
    "                                                device=device,\n",
    "                                                episode_length=max_path_length,\n",
    "                                                reward_fn=reward_fn, #Reward function to evaluate\n",
    "                                                render=False,\n",
    "                                                mpc_horizon=None,\n",
    "                                                n_samples_mpc=None)\n",
    "                sample_trajs.append(sample_traj)\n",
    "        else:\n",
    "            for it in range(num_traj_per_iter):\n",
    "                sample_traj = collect_traj_MBRL(env=env,\n",
    "                                                model=model,\n",
    "                                                plan_mode=plan_mode,\n",
    "                                                replay_buffer=replay_buffer,\n",
    "                                                device=device,\n",
    "                                                episode_length=max_path_length,\n",
    "                                                reward_fn=reward_fn, #Reward function to evaluate\n",
    "                                                render=False,\n",
    "                                                mpc_horizon=mpc_horizon,\n",
    "                                                n_samples_mpc=n_samples_mpc)\n",
    "                sample_trajs.append(sample_traj)\n",
    "\n",
    "        # Train the model\n",
    "        train_model(model, replay_buffer, optimizer, num_epochs=num_agent_train_epochs_per_iter, batch_size=batch_size)\n",
    "\n",
    "        # Logging returns occasionally\n",
    "        if iter_num % print_freq == 0:\n",
    "            rewards_np = np.mean(np.asarray([traj['rewards'].sum() for traj in sample_trajs]))\n",
    "            path_length = np.max(np.asarray([traj['rewards'].shape[0] for traj in sample_trajs]))\n",
    "            print(\"Episode: {}, reward: {}, max path length: {}\".format(iter_num, rewards_np, path_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-axaqok8NRE"
   },
   "source": [
    "# Run code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1731458623921,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "VCu8cX7QOpiR"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, model_type, plan_mode, test=False, render=False):\n",
    "        self.model_type = model_type # single or ensemble\n",
    "        self.plan_mode = plan_mode # random_mpc or mppi\n",
    "        self.test = test # whether test only\n",
    "        self.render = render # whether to render during test\n",
    "args = Args('ensemble', \"mppi\", False,  False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCAfkVpkOpiR",
    "outputId": "67dfc4cc-6110-46ce-f1de-00489c8164fb"
   },
   "outputs": [],
   "source": [
    "set_random_seed(0)\n",
    "\n",
    "# Environment and reward definition\n",
    "env = gym.make(\"Reacher-v2\")\n",
    "max_path_length=50\n",
    "\n",
    "# Define dynamics model\n",
    "hidden_dim_model = 64\n",
    "hidden_depth_model = 2\n",
    "if args.model_type == 'single':\n",
    "    model = DeterministicDynamicsModel(env.observation_space.shape[0] + env.action_space.shape[0], env.observation_space.shape[0], hidden_dim=hidden_dim_model, hidden_depth=hidden_depth_model)\n",
    "    model.to(device)\n",
    "elif args.model_type == 'ensemble':\n",
    "    num_ensembles = 5\n",
    "    model = []\n",
    "    for model_id in range(num_ensembles):\n",
    "        curr_model = DeterministicDynamicsModel(env.observation_space.shape[0] + env.action_space.shape[0], env.observation_space.shape[0], hidden_dim=hidden_dim_model, hidden_depth=hidden_depth_model)\n",
    "        curr_model.to(device)\n",
    "        model.append(curr_model)\n",
    "else:\n",
    "    raise NotImplementedError(\"No other model types implemented\")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs=15\n",
    "batch_size=250 #5000\n",
    "num_agent_train_epochs_per_iter=10 #100\n",
    "num_traj_per_iter = batch_size // max_path_length\n",
    "gamma=0.99\n",
    "print_freq=1\n",
    "capacity=100000\n",
    "mpc_horizon = 10\n",
    "n_samples_mpc = 1000\n",
    "\n",
    "if not args.test:\n",
    "    # Training and model saving code\n",
    "    simulate_mbrl(env, model, plan_mode=args.plan_mode, num_epochs=num_epochs, max_path_length=max_path_length, mpc_horizon=mpc_horizon,\n",
    "                n_samples_mpc=n_samples_mpc, batch_size=batch_size, num_agent_train_epochs_per_iter=num_agent_train_epochs_per_iter, capacity=capacity, num_traj_per_iter=num_traj_per_iter, gamma=gamma, print_freq=print_freq, device = \"cuda\", reward_fn=reward_fn_reacher)\n",
    "    if type(model) is list:\n",
    "        for model_idx, curr_model in enumerate(model):\n",
    "            torch.save(curr_model.state_dict(), f'{args.model_type}_{args.plan_mode}_{model_idx}.pth')\n",
    "    else:\n",
    "        torch.save(model.state_dict(), f'{args.model_type}_{args.plan_mode}.pth')\n",
    "else:\n",
    "    print('loading pretrained mbrl')\n",
    "    if type(model) is list:\n",
    "        for model_idx in range(len(model)):\n",
    "\n",
    "            model[model_idx].load_state_dict(torch.load(f'{args.model_type}_{args.plan_mode}_{model_idx}.pth'))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(f'{args.model_type}_{args.plan_mode}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1731458498480,
     "user": {
      "displayName": "J Berg",
      "userId": "18368992162220213123"
     },
     "user_tz": 480
    },
    "id": "3rKzEBT17-K_"
   },
   "outputs": [],
   "source": [
    "evaluate(env, model, plan_mode=args.plan_mode, mpc_horizon=mpc_horizon, n_samples_mpc=n_samples_mpc, num_validation_runs=100, episode_length=max_path_length, render=args.render, reward_fn=reward_fn_reacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5HSiAGq9_5Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
